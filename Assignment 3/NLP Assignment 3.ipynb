{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a2888efe",
      "metadata": {
        "id": "a2888efe"
      },
      "source": [
        "# NLP Assignment 3: Text Cleaning, Lemmatization, Stopwords, Encoding & TF-IDF\n",
        "\n",
        "This notebook demonstrates text preprocessing and feature extraction using NLTK and Scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e8a58997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8a58997",
        "outputId": "c2d5f4bf-4d61-41d3-cdf2-69d601bf5ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ace619c5",
      "metadata": {
        "id": "ace619c5"
      },
      "source": [
        "## Sample Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c10b0c34",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c10b0c34",
        "outputId": "e228afb1-af8f-4bc0-d9b9-afa9b61414a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love Natural Language Processing!', 'Text cleaning is an important step in NLP.', 'Machine learning models need clean data.']\n",
            "['positive', 'neutral', 'neutral']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "texts = [\n",
        "    \"I love Natural Language Processing!\",\n",
        "    \"Text cleaning is an important step in NLP.\",\n",
        "    \"Machine learning models need clean data.\"\n",
        "]\n",
        "labels = [\"positive\", \"neutral\", \"neutral\"]\n",
        "\n",
        "print(texts)\n",
        "print(labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3501d31f",
      "metadata": {
        "id": "3501d31f"
      },
      "source": [
        "## Text Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "495d48f6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "495d48f6",
        "outputId": "cebfe647-fdf3-45fe-9ff1-b798ea366f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i love natural language processing', 'text cleaning is an important step in nlp', 'machine learning models need clean data']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "cleaned_texts = [clean_text(t) for t in texts]\n",
        "print(cleaned_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d83b6785",
      "metadata": {
        "id": "d83b6785"
      },
      "source": [
        "## Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "1decb068",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1decb068",
        "outputId": "123e0e4f-67e6-4f0e-9f57-539a452b27e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i love natural language processing', 'text cleaning is an important step in nlp', 'machine learning model need clean data']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmatized_texts = []\n",
        "for text in cleaned_texts:\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    lemmatized_texts.append(\" \".join(lemmas))\n",
        "\n",
        "print(lemmatized_texts)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "944136e8",
        "outputId": "795ba650-0fff-48a8-b230-5dc80be074f8"
      },
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "id": "944136e8",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29afd5bd",
      "metadata": {
        "id": "29afd5bd"
      },
      "source": [
        "## Stopword Removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a1a03f47",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1a03f47",
        "outputId": "69e53df5-4c8f-402f-de71-90efa05829c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['love natural language processing', 'text cleaning important step nlp', 'machine learning model need clean data']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "filtered_texts = []\n",
        "for text in lemmatized_texts:\n",
        "    words = text.split()\n",
        "    filtered = [word for word in words if word not in stop_words]\n",
        "    filtered_texts.append(\" \".join(filtered))\n",
        "\n",
        "print(filtered_texts)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fae13d6a",
      "metadata": {
        "id": "fae13d6a"
      },
      "source": [
        "## Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "75861c64",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75861c64",
        "outputId": "4568f80f-2f8f-4190-85ef-6fde8d9bd298"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Labels: ['positive', 'neutral', 'neutral']\n",
            "Encoded Labels: [1 0 0]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoded_labels = encoder.fit_transform(labels)\n",
        "\n",
        "print(\"Original Labels:\", labels)\n",
        "print(\"Encoded Labels:\", encoded_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00856619",
      "metadata": {
        "id": "00856619"
      },
      "source": [
        "## TF-IDF Representation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6e67da9b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e67da9b",
        "outputId": "168efd37-4cd5-4836-fbee-8864f479e935"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Feature Names: ['clean' 'cleaning' 'data' 'important' 'language' 'learning' 'love'\n",
            " 'machine' 'model' 'natural' 'need' 'nlp' 'processing' 'step' 'text']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.         0.5        0.\n",
            "  0.5        0.         0.         0.5        0.         0.\n",
            "  0.5        0.         0.        ]\n",
            " [0.         0.4472136  0.         0.4472136  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.4472136\n",
            "  0.         0.4472136  0.4472136 ]\n",
            " [0.40824829 0.         0.40824829 0.         0.         0.40824829\n",
            "  0.         0.40824829 0.40824829 0.         0.40824829 0.\n",
            "  0.         0.         0.        ]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform(filtered_texts)\n",
        "\n",
        "print(\"TF-IDF Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4239130e",
      "metadata": {
        "id": "4239130e"
      },
      "source": [
        "## Saving Outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "69d86a8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69d86a8c",
        "outputId": "d7e57076-8d38-45df-f466-2ceb9a2cfd6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF output saved as tfidf_output.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "df['label'] = encoded_labels\n",
        "\n",
        "df.to_csv(\"tfidf_output.csv\", index=False)\n",
        "print(\"TF-IDF output saved as tfidf_output.csv\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}