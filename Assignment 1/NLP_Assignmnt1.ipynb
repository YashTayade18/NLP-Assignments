{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBkMngMmpksa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "171f1386-3995-4c67-d161-97c6ac02e273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "#Natural Language ToolKit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwTYNtU-poPi",
        "outputId": "b2b78316-12b2-42a2-b121-42fdb7791136"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, TweetTokenizer, MWETokenizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "import string\n",
        "\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7rhKJ90pq-i",
        "outputId": "9e45704e-3ead-44e4-a76d-eb594b6760d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Don't underestimate natural language processing! NLP is used in tweets like #AI and mentions @OpenAI.\n"
          ]
        }
      ],
      "source": [
        "text = \"Don't underestimate natural language processing! NLP is used in tweets like #AI and mentions @OpenAI.\"\n",
        "print(\"Original Text:\")\n",
        "print(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Whitespace** **Tokenizer**: Splits text purely based on spaces"
      ],
      "metadata": {
        "id": "rp1ONzY0wwkv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5t03nUSvpvmY",
        "outputId": "26f8ea0d-4b3b-4049-9daf-ae33c8a79762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Whitespace Tokenization:\n",
            "[\"Don't\", 'underestimate', 'natural', 'language', 'processing!', 'NLP', 'is', 'used', 'in', 'tweets', 'like', '#AI', 'and', 'mentions', '@OpenAI.']\n"
          ]
        }
      ],
      "source": [
        "def whitespace_tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "whitespace_tokens = whitespace_tokenizer(text)\n",
        "print(\"\\nWhitespace Tokenization:\")\n",
        "print(whitespace_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Punctuation Tokenizer**: Splits text at punctuation marks"
      ],
      "metadata": {
        "id": "LkLwFTYKw-GD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wEsZ4vvv7K6f",
        "outputId": "e2db44a1-1e30-4525-a43e-873e94c0d93d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation Tokenization:\n",
            "['Don', \"'\", 't underestimate natural language processing', '!', ' NLP is used in tweets like ', '#', 'AI and mentions ', '@', 'OpenAI', '.']\n"
          ]
        }
      ],
      "source": [
        "def punctuation_tokenizer(text):\n",
        "    tokens = []\n",
        "    word = \"\"\n",
        "    for char in text:\n",
        "        if char in string.punctuation:\n",
        "            if word:\n",
        "                tokens.append(word)\n",
        "                word = \"\"\n",
        "            tokens.append(char)\n",
        "        else:\n",
        "            word += char\n",
        "    if word:\n",
        "        tokens.append(word)\n",
        "    return tokens\n",
        "\n",
        "punctuation_tokens = punctuation_tokenizer(text)\n",
        "print(\"\\nPunctuation Tokenization:\")\n",
        "print(punctuation_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Treebank Tokenizer**: Handles contractions and complex punctuation correctly"
      ],
      "metadata": {
        "id": "yQiFILqPxRSg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1UDWSFx7o0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2dcc4b1-284b-4114-e5bc-9a85893be5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Tokenization:\n",
            "['Do', \"n't\", 'underestimate', 'natural', 'language', 'processing', '!', 'NLP', 'is', 'used', 'in', 'tweets', 'like', '#', 'AI', 'and', 'mentions', '@', 'OpenAI', '.']\n"
          ]
        }
      ],
      "source": [
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "\n",
        "print(\"\\nTreebank Tokenization:\")\n",
        "print(treebank_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Tweet Tokenizer**: Optimized for social media text (hashtags, mentions, emojis)"
      ],
      "metadata": {
        "id": "ZkfEnuubxbMC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "\n",
        "print(\"\\nTweet Tokenization:\")\n",
        "print(tweet_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2SKkv6pxXlu",
        "outputId": "4b523f59-c42f-49bc-942d-c608db4bcd53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenization:\n",
            "[\"Don't\", 'underestimate', 'natural', 'language', 'processing', '!', 'NLP', 'is', 'used', 'in', 'tweets', 'like', '#AI', 'and', 'mentions', '@OpenAI', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. MWE (Multi-Word Expression) Tokenizer**: Identifies and tokenizes multi-word expressions as a single unit"
      ],
      "metadata": {
        "id": "SvN-2aQ0xg4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "mwe_tokenizer = MWETokenizer([('natural', 'language'), ('language', 'processing')], separator='_')\n",
        "\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
        "print(\"\\nMWE Tokenization:\")\n",
        "print(mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H1RDIR5xgP5",
        "outputId": "08c0812f-5326-45b9-cc36-f08cba0687f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "MWE Tokenization:\n",
            "['Do', \"n't\", 'underestimate', 'natural_language', 'processing', '!', 'NLP', 'is', 'used', 'in', 'tweets', 'like', '#', 'AI', 'and', 'mentions', '@', 'OpenAI', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2] What is Stemming?\n",
        "\n",
        "Stemming is a text normalization technique that removes suffixes (and occasionally prefixes) from words to obtain their stem. The resulting stem may not be a valid dictionary word, but it represents a common linguistic root shared across word variants."
      ],
      "metadata": {
        "id": "QNRh5ECO5Va3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "id": "MdTNMJBNxqzN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da818981-31e9-42dc-f6d6-cb51418b662f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Stemming algorithms are useful for reducing words like running, runs, and runner into a common root.\"\n",
        "print(\"Original Text:\")\n",
        "print(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqMponhR486A",
        "outputId": "9c6a1b22-cc29-4abd-bbb6-26712d0e5a9c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Stemming algorithms are useful for reducing words like running, runs, and runner into a common root.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\nTokens:\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_GRf63q6tk1",
        "outputId": "1811eeec-cbd7-4de1-ec55-c7e585a9d2ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokens:\n",
            "['Stemming', 'algorithms', 'are', 'useful', 'for', 'reducing', 'words', 'like', 'running', ',', 'runs', ',', 'and', 'runner', 'into', 'a', 'common', 'root', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Porter Stemmer**: The Porter Stemmer is a rule-based stemming algorithm. It applies a sequence of deterministic rules to remove common English suffixes."
      ],
      "metadata": {
        "id": "vZ7IZPp75Gpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()\n",
        "porter_stems = [porter.stem(word) for word in tokens]\n",
        "\n",
        "print(\"\\nPorter Stemmer Output:\")\n",
        "print(porter_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-SqC09I5FKG",
        "outputId": "a5b6d958-22eb-45bb-ceea-73116b875e40"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Porter Stemmer Output:\n",
            "['stem', 'algorithm', 'are', 'use', 'for', 'reduc', 'word', 'like', 'run', ',', 'run', ',', 'and', 'runner', 'into', 'a', 'common', 'root', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Snowball Stemmer: The Snowball Stemmer is an improved version of the Porter algorithm with better rules and multi-language support."
      ],
      "metadata": {
        "id": "MlKePaS-5fWr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "snowball = SnowballStemmer(\"english\")\n",
        "snowball_stems = [snowball.stem(word) for word in tokens]\n",
        "\n",
        "print(\"\\nSnowball Stemmer Output:\")\n",
        "print(snowball_stems)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ns_NtVAJ5KVk",
        "outputId": "4c7d1ef9-642c-4933-b62f-70096ff4fb84"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Snowball Stemmer Output:\n",
            "['stem', 'algorithm', 'are', 'use', 'for', 'reduc', 'word', 'like', 'run', ',', 'run', ',', 'and', 'runner', 'into', 'a', 'common', 'root', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Lemmatization (WordNet Lemmatizer)\n",
        "\n",
        "\n",
        "    Lemmatization is a linguistically informed text normalization technique that reduces words to their base dictionary form, known as a lemma, by considering the wordâ€™s part of speech and contextual meaning."
      ],
      "metadata": {
        "id": "pqLxr3qZ5ypE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WordNet Lemmatizer\n",
        "\n",
        "WordNet is a large lexical database of English that groups words into sets of synonyms (synsets) and defines semantic relationships among them."
      ],
      "metadata": {
        "id": "sO690Amk5-Cw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98rKYb8f5hv8",
        "outputId": "513c14a9-ea70-4ae2-e2a0-b59cf5e3bacf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "83uoMFq76DH5"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n"
      ],
      "metadata": {
        "id": "cKrnnvky6NuE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "lemmatized_words = [\n",
        "    lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "    for word, pos in pos_tags\n",
        "]\n",
        "\n",
        "print(\"\\nWordNet Lemmatization (With POS Tags):\")\n",
        "print(lemmatized_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYVUoqx96P_a",
        "outputId": "610b1589-d4f5-4f33-f9ce-00482be75e28"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WordNet Lemmatization (With POS Tags):\n",
            "['Stemming', 'algorithm', 'be', 'useful', 'for', 'reduce', 'word', 'like', 'run', ',', 'run', ',', 'and', 'runner', 'into', 'a', 'common', 'root', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MoqiivKF6wmu"
      },
      "execution_count": 22,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}