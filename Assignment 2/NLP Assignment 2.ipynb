{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9e587d5f",
      "metadata": {
        "id": "9e587d5f"
      },
      "source": [
        "# NLP Assignment 2: Bag of Words, TF-IDF, and Word2Vec\n",
        "\n",
        "This notebook demonstrates Bag-of-Words, TF-IDF, and Word2Vec embeddings using Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ef9e9573",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef9e9573",
        "outputId": "85b2ab2c-a367-4a01-f982-807b1c69ae2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b9e4f9",
      "metadata": {
        "id": "f4b9e4f9"
      },
      "source": [
        "## Sample Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c9ad5b20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9ad5b20",
        "outputId": "2b631b13-e38d-4f8b-ab65-c25089ab118c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I love natural language processing', 'Natural language processing is fun', 'I love learning new AI techniques']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "corpus = [\n",
        "    \"I love natural language processing\",\n",
        "    \"Natural language processing is fun\",\n",
        "    \"I love learning new AI techniques\"\n",
        "]\n",
        "\n",
        "print(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0403a65f",
      "metadata": {
        "id": "0403a65f"
      },
      "source": [
        "## Bag of Words (Count Occurrence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3d712716",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d712716",
        "outputId": "a8b35fb8-75a5-40bb-ff93-69731521e048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['ai' 'fun' 'is' 'language' 'learning' 'love' 'natural' 'new' 'processing'\n",
            " 'techniques']\n",
            "Count Occurrence Matrix:\n",
            " [[0 0 0 1 0 1 1 0 1 0]\n",
            " [0 1 1 1 0 0 1 0 1 0]\n",
            " [1 0 0 0 1 1 0 1 0 1]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_counts = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", count_vectorizer.get_feature_names_out())\n",
        "print(\"Count Occurrence Matrix:\\n\", bow_counts.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5371d3f",
      "metadata": {
        "id": "b5371d3f"
      },
      "source": [
        "## Bag of Words (Normalized Count Occurrence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a7eed25e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7eed25e",
        "outputId": "20f435a3-4653-425c-8e14-2c9484b5a4d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['ai' 'fun' 'is' 'language' 'learning' 'love' 'natural' 'new' 'processing'\n",
            " 'techniques']\n",
            "Normalized BoW Matrix:\n",
            " [[0.   0.   0.   0.25 0.   0.25 0.25 0.   0.25 0.  ]\n",
            " [0.   0.2  0.2  0.2  0.   0.   0.2  0.   0.2  0.  ]\n",
            " [0.2  0.   0.   0.   0.2  0.2  0.   0.2  0.   0.2 ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import normalize # Import normalize function\n",
        "\n",
        "# First, get the raw count occurrence matrix using CountVectorizer\n",
        "# A new instance of CountVectorizer is created for this section to maintain clarity\n",
        "count_vectorizer_for_norm = CountVectorizer()\n",
        "raw_bow_counts = count_vectorizer_for_norm.fit_transform(corpus)\n",
        "\n",
        "# Then, apply L1 normalization to the raw count matrix\n",
        "# 'axis=1' normalizes each document's vector (row) independently\n",
        "bow_normalized = normalize(raw_bow_counts, norm='l1', axis=1)\n",
        "\n",
        "print(\"Vocabulary:\", count_vectorizer_for_norm.get_feature_names_out())\n",
        "print(\"Normalized BoW Matrix:\\n\", bow_normalized.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df7ba540",
      "metadata": {
        "id": "df7ba540"
      },
      "source": [
        "## TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "06598c4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "06598c4f",
        "outputId": "43e1f0ed-cd35-41a2-bb71-f93b0e6f0e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['ai' 'fun' 'is' 'language' 'learning' 'love' 'natural' 'new' 'processing'\n",
            " 'techniques']\n",
            "TF-IDF Matrix:\n",
            " [[0.         0.         0.         0.5        0.         0.5\n",
            "  0.5        0.         0.5        0.        ]\n",
            " [0.         0.51741994 0.51741994 0.3935112  0.         0.\n",
            "  0.3935112  0.         0.3935112  0.        ]\n",
            " [0.46735098 0.         0.         0.         0.46735098 0.35543247\n",
            "  0.         0.46735098 0.         0.46735098]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Matrix:\\n\", tfidf_matrix.toarray())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6eb123",
      "metadata": {
        "id": "1c6eb123"
      },
      "source": [
        "## Word2Vec Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "b8ec427a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ec427a",
        "outputId": "2032c4c6-6f7a-4d5b-cbf4-a87f18677528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Vector for 'language':\n",
            " [-0.01631583  0.0089916  -0.00827415  0.00164907  0.01699724 -0.00892435\n",
            "  0.009035   -0.01357392 -0.00709698  0.01879702 -0.00315531  0.00064274\n",
            " -0.00828126 -0.01536538 -0.00301602  0.00493959 -0.00177605  0.01106732\n",
            " -0.00548595  0.00452013  0.01091159  0.01669191 -0.00290748 -0.01841629\n",
            "  0.0087411   0.00114357  0.01488382 -0.00162657 -0.00527683 -0.01750602\n",
            " -0.00171311  0.00565313  0.01080286  0.01410531 -0.01140624  0.00371764\n",
            "  0.01217773 -0.0095961  -0.00621452  0.01359526  0.00326295  0.00037983\n",
            "  0.00694727  0.00043555  0.01923765  0.01012121 -0.01783478 -0.01408312\n",
            "  0.00180291  0.01278507]\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_corpus = [word_tokenize(sentence.lower()) for sentence in corpus]\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=tokenized_corpus,\n",
        "    vector_size=50,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=2\n",
        ")\n",
        "\n",
        "print(\"Word Vector for 'language':\\n\", w2v_model.wv['language'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}